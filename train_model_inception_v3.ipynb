{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import h5py\n",
    "import math\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "#from numba import cuda \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "#sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiling.read_tiles import TissueDataset, load_color_normalization_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snippet nvidia gpu\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "#config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDF5_FOLDER = '/home/sarah/ForthBrainCancer-Dataset/training_CAMELYON16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "62\n",
      "40\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "train_data = TissueDataset(HDF5_FOLDER,  percentage=0.8, first_part=True, crop_size=256)\n",
    "val_data = TissueDataset(HDF5_FOLDER, percentage=0.2, first_part=False, crop_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/home/sarah/ForthBrainCancer-Dataset/inception_logs\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_neg=50\n",
    "batch_size_pos=50\n",
    "batches_per_train_epoch = 100\n",
    "batches_per_val_epoch = 50\n",
    "MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_normalization_file=\"CAMELYON16_color_normalization.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_to_pickle(history, filepath):\n",
    "    with open(filepath, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "        \n",
    "def pickle_to_history(filepath):\n",
    "    history = pickle.load(open(filepath, \"rb\"))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # list all data in history\n",
    "    print(history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for precision\n",
    "    plt.plot(history['precision'])\n",
    "    plt.plot(history['val_precision'])\n",
    "    plt.title('model precision')\n",
    "    plt.ylabel('precision')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for recall\n",
    "    plt.plot(history['recall'])\n",
    "    plt.plot(history['val_recall'])\n",
    "    plt.title('model recall')\n",
    "    plt.ylabel('recall')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.InceptionV3(input_shape=(256,256,3),\n",
    "                                        include_top=True,\n",
    "                                        classes = 1,\n",
    "                                        classifier_activation=\"sigmoid\",\n",
    "                                        weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception_v3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 127, 127, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 127, 127, 32) 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 127, 127, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 125, 125, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 125, 125, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 125, 125, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 125, 125, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 125, 125, 64) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 125, 125, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 62, 62, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 62, 62, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 62, 62, 80)   240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 62, 62, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 60, 60, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 60, 60, 192)  576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 60, 60, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 29, 29, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 29, 29, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 29, 29, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 29, 29, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 29, 29, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 29, 29, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 29, 29, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 29, 29, 96)   288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 29, 29, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 29, 29, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 29, 29, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 29, 29, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 29, 29, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 29, 29, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 29, 29, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 29, 29, 64)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 29, 29, 64)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 29, 29, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 29, 29, 32)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 29, 29, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 29, 29, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 29, 29, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 29, 29, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 29, 29, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 29, 29, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 29, 29, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 29, 29, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 29, 29, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 29, 29, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 29, 29, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 29, 29, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 29, 29, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 29, 29, 96)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 29, 29, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 29, 29, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 29, 29, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 29, 29, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 29, 29, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 29, 29, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 29, 29, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 29, 29, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 29, 29, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 29, 29, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 29, 29, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 29, 29, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 29, 29, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 29, 29, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 29, 29, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 29, 29, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 29, 29, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 29, 29, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 29, 29, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 29, 29, 48)   144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 29, 29, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 29, 29, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 29, 29, 96)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 29, 29, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 29, 29, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 29, 29, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 29, 29, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 29, 29, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 29, 29, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 29, 29, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 29, 29, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 29, 29, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 29, 29, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 29, 29, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 29, 29, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 29, 29, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 29, 29, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 29, 29, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 29, 29, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 29, 29, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 29, 29, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 29, 29, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 29, 29, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 14, 14, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 14, 14, 96)   82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 14, 14, 384)  1152        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 14, 14, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 384)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 14, 14, 768)  0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 14, 14, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 14, 14, 128)  384         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 14, 14, 128)  114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 14, 14, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 14, 14, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 14, 14, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 14, 14, 128)  384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 14, 14, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 14, 14, 128)  114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 14, 14, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 14, 14, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 14, 14, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 14, 14, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 14, 14, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 14, 14, 192)  172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 14, 14, 192)  172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 14, 14, 192)  576         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 14, 14, 192)  576         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 14, 14, 192)  576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 14, 14, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 192)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 192)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 192)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 14, 14, 768)  0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 14, 14, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 14, 14, 160)  480         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 14, 14, 160)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 14, 14, 160)  179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 14, 14, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 14, 14, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 14, 14, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 14, 14, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 14, 14, 160)  480         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 14, 14, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 14, 14, 160)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 14, 14, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 14, 14, 160)  179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 14, 14, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 14, 14, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 14, 14, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 14, 14, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 14, 14, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 14, 14, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 14, 14, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 14, 14, 192)  215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 14, 14, 192)  215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 14, 14, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 14, 14, 192)  576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 14, 14, 192)  576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 14, 14, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 14, 14, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 14, 14, 192)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 14, 14, 192)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 14, 14, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 14, 14, 768)  0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 14, 14, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 14, 14, 160)  480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 14, 14, 160)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 14, 14, 160)  179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 14, 14, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 14, 14, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 14, 14, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 14, 14, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 14, 14, 160)  480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 14, 14, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 14, 14, 160)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 14, 14, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 14, 14, 160)  179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 14, 14, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 14, 14, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 14, 14, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 14, 14, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 14, 14, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 14, 14, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 14, 14, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 14, 14, 192)  215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 14, 14, 192)  215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 14, 14, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 14, 14, 192)  576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 14, 14, 192)  576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 14, 14, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 14, 14, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 14, 14, 192)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 14, 14, 192)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 14, 14, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 14, 14, 768)  0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 14, 14, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 14, 14, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 14, 14, 192)  258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 14, 14, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 14, 14, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 14, 14, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 14, 14, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 14, 14, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 14, 14, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 14, 14, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 14, 14, 192)  258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 14, 14, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 14, 14, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 14, 14, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 14, 14, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 14, 14, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 14, 14, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 14, 14, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 14, 14, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 14, 14, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 14, 14, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 14, 14, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 14, 14, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 14, 14, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 14, 14, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 14, 14, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 14, 14, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 14, 14, 768)  0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 14, 14, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 14, 14, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 14, 14, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 14, 14, 192)  258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 14, 14, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 14, 14, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 14, 14, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 14, 14, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 14, 14, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 14, 14, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 14, 14, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 14, 14, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 6, 6, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 6, 6, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 6, 6, 320)    960         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 6, 6, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 6, 6, 320)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 6, 6, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 6, 6, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 6, 6, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 6, 6, 448)    1344        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 6, 6, 448)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 6, 6, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 6, 6, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 6, 6, 384)    1152        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 6, 6, 384)    1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 6, 6, 384)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 6, 6, 384)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 6, 6, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 6, 6, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 6, 6, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 6, 6, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 6, 6, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 6, 6, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 6, 6, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 6, 6, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 6, 6, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 6, 6, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 6, 6, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 6, 6, 320)    960         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 6, 6, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 6, 6, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 6, 6, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 6, 6, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 6, 6, 192)    576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 6, 6, 320)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 6, 6, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6, 6, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 6, 6, 192)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 6, 6, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 6, 6, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 6, 6, 448)    1344        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 6, 6, 448)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 6, 6, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 6, 6, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 6, 6, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 6, 6, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 6, 6, 384)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 6, 6, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 6, 6, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 6, 6, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 6, 6, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 6, 6, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 6, 6, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 6, 6, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 6, 6, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 6, 6, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 6, 6, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 6, 6, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 6, 6, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 6, 6, 320)    960         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 6, 6, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 6, 6, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 6, 6, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 6, 6, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 6, 6, 192)    576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 6, 6, 320)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 6, 6, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6, 6, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 6, 6, 192)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 6, 6, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 1)            2049        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 21,804,833\n",
      "Trainable params: 21,770,401\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 2474s 25s/step - loss: 0.2556 - accuracy: 0.8910 - precision: 0.9131 - recall: 0.8604 - val_loss: 0.6985 - val_accuracy: 0.5498 - val_precision: 0.5262 - val_recall: 0.9988\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.19517, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 2475s 25s/step - loss: 0.1572 - accuracy: 0.9343 - precision: 0.9434 - recall: 0.9241 - val_loss: 1.0728 - val_accuracy: 0.3934 - val_precision: 0.2799 - val_recall: 0.1356\n",
      "\n",
      "Epoch 00002: loss improved from 0.19517 to 0.15373, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 2475s 25s/step - loss: 0.1450 - accuracy: 0.9425 - precision: 0.9537 - recall: 0.9303 - val_loss: 2.0079 - val_accuracy: 0.5488 - val_precision: 0.5259 - val_recall: 0.9924\n",
      "\n",
      "Epoch 00003: loss improved from 0.15373 to 0.14035, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 2469s 25s/step - loss: 0.1403 - accuracy: 0.9463 - precision: 0.9521 - recall: 0.9400 - val_loss: 0.6252 - val_accuracy: 0.7920 - val_precision: 0.7408 - val_recall: 0.8984\n",
      "\n",
      "Epoch 00004: loss improved from 0.14035 to 0.13112, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 2471s 25s/step - loss: 0.1178 - accuracy: 0.9562 - precision: 0.9615 - recall: 0.9505 - val_loss: 0.4598 - val_accuracy: 0.8714 - val_precision: 0.8669 - val_recall: 0.8776\n",
      "\n",
      "Epoch 00005: loss improved from 0.13112 to 0.11566, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 2487s 25s/step - loss: 0.1147 - accuracy: 0.9566 - precision: 0.9640 - recall: 0.9486 - val_loss: 0.2757 - val_accuracy: 0.9046 - val_precision: 0.9087 - val_recall: 0.8996\n",
      "\n",
      "Epoch 00006: loss improved from 0.11566 to 0.11290, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 2598s 26s/step - loss: 0.1106 - accuracy: 0.9581 - precision: 0.9667 - recall: 0.9488 - val_loss: 0.4414 - val_accuracy: 0.8620 - val_precision: 0.8670 - val_recall: 0.8552\n",
      "\n",
      "Epoch 00007: loss improved from 0.11290 to 0.10778, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 2677s 27s/step - loss: 0.0943 - accuracy: 0.9650 - precision: 0.9722 - recall: 0.9575 - val_loss: 0.2653 - val_accuracy: 0.9102 - val_precision: 0.8913 - val_recall: 0.9344\n",
      "\n",
      "Epoch 00008: loss improved from 0.10778 to 0.09185, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 2687s 27s/step - loss: 0.1042 - accuracy: 0.9600 - precision: 0.9661 - recall: 0.9536 - val_loss: 0.3097 - val_accuracy: 0.9056 - val_precision: 0.9639 - val_recall: 0.8428\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.09185\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 3120s 31s/step - loss: 0.0867 - accuracy: 0.9675 - precision: 0.9732 - recall: 0.9616 - val_loss: 1.0752 - val_accuracy: 0.7670 - val_precision: 0.6991 - val_recall: 0.9376\n",
      "\n",
      "Epoch 00010: loss improved from 0.09185 to 0.09003, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 4463s 45s/step - loss: 0.0908 - accuracy: 0.9666 - precision: 0.9722 - recall: 0.9606 - val_loss: 0.3731 - val_accuracy: 0.8906 - val_precision: 0.9429 - val_recall: 0.8316\n",
      "\n",
      "Epoch 00011: loss improved from 0.09003 to 0.08769, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 4223s 42s/step - loss: 0.0939 - accuracy: 0.9658 - precision: 0.9723 - recall: 0.9588 - val_loss: 0.4252 - val_accuracy: 0.8786 - val_precision: 0.9660 - val_recall: 0.7848\n",
      "\n",
      "Epoch 00012: loss did not improve from 0.08769\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 4423s 44s/step - loss: 0.0752 - accuracy: 0.9734 - precision: 0.9769 - recall: 0.9697 - val_loss: 0.2121 - val_accuracy: 0.9272 - val_precision: 0.9615 - val_recall: 0.8900\n",
      "\n",
      "Epoch 00013: loss improved from 0.08769 to 0.07853, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 3876s 39s/step - loss: 0.0778 - accuracy: 0.9702 - precision: 0.9745 - recall: 0.9656 - val_loss: 0.3089 - val_accuracy: 0.8996 - val_precision: 0.9201 - val_recall: 0.8752\n",
      "\n",
      "Epoch 00014: loss improved from 0.07853 to 0.07635, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 2649s 26s/step - loss: 0.0703 - accuracy: 0.9745 - precision: 0.9792 - recall: 0.9697 - val_loss: 0.6614 - val_accuracy: 0.8542 - val_precision: 0.9421 - val_recall: 0.7548\n",
      "\n",
      "Epoch 00015: loss improved from 0.07635 to 0.07369, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 2669s 27s/step - loss: 0.0805 - accuracy: 0.9692 - precision: 0.9740 - recall: 0.9641 - val_loss: 0.3227 - val_accuracy: 0.8880 - val_precision: 0.8985 - val_recall: 0.8748\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.07369\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 2666s 27s/step - loss: 0.0876 - accuracy: 0.9654 - precision: 0.9709 - recall: 0.9596 - val_loss: 0.3405 - val_accuracy: 0.9136 - val_precision: 0.9045 - val_recall: 0.9248\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.07369\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 2672s 27s/step - loss: 0.0705 - accuracy: 0.9746 - precision: 0.9784 - recall: 0.9705 - val_loss: 0.6150 - val_accuracy: 0.8618 - val_precision: 0.9467 - val_recall: 0.7668\n",
      "\n",
      "Epoch 00018: loss improved from 0.07369 to 0.07212, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 2680s 27s/step - loss: 0.0747 - accuracy: 0.9740 - precision: 0.9773 - recall: 0.9705 - val_loss: 0.4569 - val_accuracy: 0.8632 - val_precision: 0.9399 - val_recall: 0.7760\n",
      "\n",
      "Epoch 00019: loss improved from 0.07212 to 0.06752, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 2669s 27s/step - loss: 0.0611 - accuracy: 0.9764 - precision: 0.9799 - recall: 0.9727 - val_loss: 0.2956 - val_accuracy: 0.9122 - val_precision: 0.9686 - val_recall: 0.8520\n",
      "\n",
      "Epoch 00020: loss improved from 0.06752 to 0.05774, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 2677s 27s/step - loss: 0.0627 - accuracy: 0.9767 - precision: 0.9773 - recall: 0.9761 - val_loss: 0.3231 - val_accuracy: 0.9078 - val_precision: 0.9289 - val_recall: 0.8832\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.05774\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 2659s 27s/step - loss: 0.0578 - accuracy: 0.9793 - precision: 0.9822 - recall: 0.9764 - val_loss: 0.2694 - val_accuracy: 0.9178 - val_precision: 0.9180 - val_recall: 0.9176\n",
      "\n",
      "Epoch 00022: loss improved from 0.05774 to 0.05604, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 2671s 27s/step - loss: 0.0542 - accuracy: 0.9786 - precision: 0.9832 - recall: 0.9739 - val_loss: 0.2717 - val_accuracy: 0.9204 - val_precision: 0.9039 - val_recall: 0.9408\n",
      "\n",
      "Epoch 00023: loss improved from 0.05604 to 0.05232, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 2673s 27s/step - loss: 0.0435 - accuracy: 0.9853 - precision: 0.9862 - recall: 0.9845 - val_loss: 0.3558 - val_accuracy: 0.9092 - val_precision: 0.9475 - val_recall: 0.8664\n",
      "\n",
      "Epoch 00024: loss improved from 0.05232 to 0.04509, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 2670s 27s/step - loss: 0.0446 - accuracy: 0.9829 - precision: 0.9863 - recall: 0.9793 - val_loss: 0.2620 - val_accuracy: 0.9206 - val_precision: 0.9174 - val_recall: 0.9244\n",
      "\n",
      "Epoch 00025: loss improved from 0.04509 to 0.04500, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 2668s 27s/step - loss: 0.0400 - accuracy: 0.9856 - precision: 0.9856 - recall: 0.9856 - val_loss: 0.2963 - val_accuracy: 0.9122 - val_precision: 0.9191 - val_recall: 0.9040\n",
      "\n",
      "Epoch 00026: loss improved from 0.04500 to 0.04248, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 2677s 27s/step - loss: 0.0456 - accuracy: 0.9853 - precision: 0.9861 - recall: 0.9844 - val_loss: 0.2324 - val_accuracy: 0.9262 - val_precision: 0.9513 - val_recall: 0.8984\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.04248\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 2660s 27s/step - loss: 0.0440 - accuracy: 0.9819 - precision: 0.9840 - recall: 0.9797 - val_loss: 0.3272 - val_accuracy: 0.9094 - val_precision: 0.9467 - val_recall: 0.8676\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.04248\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 2657s 27s/step - loss: 0.0419 - accuracy: 0.9826 - precision: 0.9854 - recall: 0.9798 - val_loss: 0.3063 - val_accuracy: 0.9238 - val_precision: 0.9565 - val_recall: 0.8880\n",
      "\n",
      "Epoch 00029: loss improved from 0.04248 to 0.04063, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 2688s 27s/step - loss: 0.0417 - accuracy: 0.9836 - precision: 0.9863 - recall: 0.9809 - val_loss: 0.2790 - val_accuracy: 0.9230 - val_precision: 0.9521 - val_recall: 0.8908\n",
      "\n",
      "Epoch 00030: loss improved from 0.04063 to 0.04016, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 3885s 39s/step - loss: 0.0353 - accuracy: 0.9870 - precision: 0.9890 - recall: 0.9850 - val_loss: 0.2578 - val_accuracy: 0.9240 - val_precision: 0.9507 - val_recall: 0.8944\n",
      "\n",
      "Epoch 00031: loss improved from 0.04016 to 0.03598, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 2660s 27s/step - loss: 0.0355 - accuracy: 0.9853 - precision: 0.9898 - recall: 0.9807 - val_loss: 0.3662 - val_accuracy: 0.9020 - val_precision: 0.9001 - val_recall: 0.9044\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.03598\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 2659s 27s/step - loss: 0.0407 - accuracy: 0.9855 - precision: 0.9865 - recall: 0.9844 - val_loss: 0.2722 - val_accuracy: 0.9268 - val_precision: 0.9424 - val_recall: 0.9092\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.03598\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 2639s 26s/step - loss: 0.0402 - accuracy: 0.9854 - precision: 0.9888 - recall: 0.9820 - val_loss: 0.2971 - val_accuracy: 0.9204 - val_precision: 0.9420 - val_recall: 0.8960\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.03598\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 2658s 27s/step - loss: 0.0351 - accuracy: 0.9870 - precision: 0.9890 - recall: 0.9849 - val_loss: 0.3027 - val_accuracy: 0.9144 - val_precision: 0.9225 - val_recall: 0.9048\n",
      "\n",
      "Epoch 00035: loss improved from 0.03598 to 0.03403, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 2662s 27s/step - loss: 0.0385 - accuracy: 0.9859 - precision: 0.9899 - recall: 0.9819 - val_loss: 0.3263 - val_accuracy: 0.9144 - val_precision: 0.9309 - val_recall: 0.8952\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.03403\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 2660s 27s/step - loss: 0.0376 - accuracy: 0.9877 - precision: 0.9922 - recall: 0.9830 - val_loss: 0.2541 - val_accuracy: 0.9304 - val_precision: 0.9403 - val_recall: 0.9192\n",
      "\n",
      "Epoch 00037: loss improved from 0.03403 to 0.03310, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 2664s 27s/step - loss: 0.0349 - accuracy: 0.9864 - precision: 0.9878 - recall: 0.9850 - val_loss: 0.3028 - val_accuracy: 0.9174 - val_precision: 0.9515 - val_recall: 0.8796\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.03310\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 2662s 27s/step - loss: 0.0338 - accuracy: 0.9885 - precision: 0.9923 - recall: 0.9847 - val_loss: 0.2683 - val_accuracy: 0.9280 - val_precision: 0.9492 - val_recall: 0.9044\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.03310\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 2651s 27s/step - loss: 0.0318 - accuracy: 0.9878 - precision: 0.9902 - recall: 0.9855 - val_loss: 0.3924 - val_accuracy: 0.9042 - val_precision: 0.9339 - val_recall: 0.8700\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.03310\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 2669s 27s/step - loss: 0.0368 - accuracy: 0.9852 - precision: 0.9891 - recall: 0.9813 - val_loss: 0.3499 - val_accuracy: 0.9072 - val_precision: 0.9252 - val_recall: 0.8860\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.03310\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 2664s 27s/step - loss: 0.0339 - accuracy: 0.9877 - precision: 0.9897 - recall: 0.9857 - val_loss: 0.3194 - val_accuracy: 0.9168 - val_precision: 0.9267 - val_recall: 0.9052\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.03310\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 2658s 27s/step - loss: 0.0348 - accuracy: 0.9869 - precision: 0.9882 - recall: 0.9856 - val_loss: 0.3296 - val_accuracy: 0.9140 - val_precision: 0.9675 - val_recall: 0.8568\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.03310\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 2655s 27s/step - loss: 0.0308 - accuracy: 0.9870 - precision: 0.9895 - recall: 0.9845 - val_loss: 0.3090 - val_accuracy: 0.9234 - val_precision: 0.9564 - val_recall: 0.8872\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.03310\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 2667s 27s/step - loss: 0.0266 - accuracy: 0.9896 - precision: 0.9917 - recall: 0.9875 - val_loss: 0.3171 - val_accuracy: 0.9250 - val_precision: 0.9741 - val_recall: 0.8732\n",
      "\n",
      "Epoch 00045: loss improved from 0.03310 to 0.02945, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 2665s 27s/step - loss: 0.0309 - accuracy: 0.9886 - precision: 0.9908 - recall: 0.9864 - val_loss: 0.3614 - val_accuracy: 0.9138 - val_precision: 0.9412 - val_recall: 0.8828\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.02945\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 2650s 27s/step - loss: 0.0365 - accuracy: 0.9858 - precision: 0.9857 - recall: 0.9859 - val_loss: 0.3451 - val_accuracy: 0.9148 - val_precision: 0.9383 - val_recall: 0.8880\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.02945\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 2654s 27s/step - loss: 0.0289 - accuracy: 0.9881 - precision: 0.9902 - recall: 0.9859 - val_loss: 0.3641 - val_accuracy: 0.9102 - val_precision: 0.9033 - val_recall: 0.9188\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.02945\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 2655s 27s/step - loss: 0.0286 - accuracy: 0.9900 - precision: 0.9919 - recall: 0.9880 - val_loss: 0.3133 - val_accuracy: 0.9192 - val_precision: 0.9142 - val_recall: 0.9252\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.02945\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 2643s 26s/step - loss: 0.0297 - accuracy: 0.9879 - precision: 0.9888 - recall: 0.9869 - val_loss: 0.2981 - val_accuracy: 0.9210 - val_precision: 0.9294 - val_recall: 0.9112\n",
      "\n",
      "Epoch 00050: loss improved from 0.02945 to 0.02908, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 2658s 27s/step - loss: 0.0315 - accuracy: 0.9890 - precision: 0.9900 - recall: 0.9879 - val_loss: 0.3115 - val_accuracy: 0.9270 - val_precision: 0.9468 - val_recall: 0.9048\n",
      "\n",
      "Epoch 00051: loss improved from 0.02908 to 0.02718, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 2671s 27s/step - loss: 0.0274 - accuracy: 0.9899 - precision: 0.9911 - recall: 0.9886 - val_loss: 0.3677 - val_accuracy: 0.9192 - val_precision: 0.9433 - val_recall: 0.8920\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.02718\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 2659s 27s/step - loss: 0.0360 - accuracy: 0.9857 - precision: 0.9867 - recall: 0.9847 - val_loss: 0.3244 - val_accuracy: 0.9208 - val_precision: 0.9376 - val_recall: 0.9016\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.02718\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 2667s 27s/step - loss: 0.0360 - accuracy: 0.9885 - precision: 0.9885 - recall: 0.9884 - val_loss: 0.2394 - val_accuracy: 0.9368 - val_precision: 0.9546 - val_recall: 0.9172\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.02718\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 2669s 27s/step - loss: 0.0289 - accuracy: 0.9897 - precision: 0.9893 - recall: 0.9902 - val_loss: 0.3247 - val_accuracy: 0.9172 - val_precision: 0.9339 - val_recall: 0.8980\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.02718\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 2675s 27s/step - loss: 0.0341 - accuracy: 0.9866 - precision: 0.9869 - recall: 0.9863 - val_loss: 0.3625 - val_accuracy: 0.9152 - val_precision: 0.9494 - val_recall: 0.8772\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.02718\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 2668s 27s/step - loss: 0.0358 - accuracy: 0.9857 - precision: 0.9884 - recall: 0.9829 - val_loss: 0.3526 - val_accuracy: 0.9150 - val_precision: 0.9517 - val_recall: 0.8744\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.02718\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 2675s 27s/step - loss: 0.0271 - accuracy: 0.9900 - precision: 0.9918 - recall: 0.9883 - val_loss: 0.4213 - val_accuracy: 0.9096 - val_precision: 0.9638 - val_recall: 0.8512\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.02718\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 2672s 27s/step - loss: 0.0280 - accuracy: 0.9893 - precision: 0.9910 - recall: 0.9875 - val_loss: 0.3533 - val_accuracy: 0.9140 - val_precision: 0.9442 - val_recall: 0.8800\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.02718\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 2687s 27s/step - loss: 0.0261 - accuracy: 0.9893 - precision: 0.9891 - recall: 0.9895 - val_loss: 0.2984 - val_accuracy: 0.9200 - val_precision: 0.9487 - val_recall: 0.8880\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.02718\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 2673s 27s/step - loss: 0.0318 - accuracy: 0.9884 - precision: 0.9884 - recall: 0.9884 - val_loss: 0.2932 - val_accuracy: 0.9272 - val_precision: 0.9648 - val_recall: 0.8868\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.02718\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 2685s 27s/step - loss: 0.0254 - accuracy: 0.9907 - precision: 0.9933 - recall: 0.9881 - val_loss: 0.3130 - val_accuracy: 0.9234 - val_precision: 0.9600 - val_recall: 0.8836\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.02718\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 2677s 27s/step - loss: 0.0261 - accuracy: 0.9912 - precision: 0.9934 - recall: 0.9890 - val_loss: 0.3273 - val_accuracy: 0.9256 - val_precision: 0.9582 - val_recall: 0.8900\n",
      "\n",
      "Epoch 00063: loss improved from 0.02718 to 0.02347, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 2656s 27s/step - loss: 0.0348 - accuracy: 0.9871 - precision: 0.9887 - recall: 0.9855 - val_loss: 0.3610 - val_accuracy: 0.9152 - val_precision: 0.9343 - val_recall: 0.8932\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.02347\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 2660s 27s/step - loss: 0.0285 - accuracy: 0.9883 - precision: 0.9902 - recall: 0.9864 - val_loss: 0.2671 - val_accuracy: 0.9340 - val_precision: 0.9436 - val_recall: 0.9232\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.02347\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 2642s 26s/step - loss: 0.0251 - accuracy: 0.9910 - precision: 0.9919 - recall: 0.9901 - val_loss: 0.3133 - val_accuracy: 0.9260 - val_precision: 0.9343 - val_recall: 0.9164\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.02347\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 2657s 27s/step - loss: 0.0294 - accuracy: 0.9889 - precision: 0.9897 - recall: 0.9881 - val_loss: 0.3198 - val_accuracy: 0.9206 - val_precision: 0.9379 - val_recall: 0.9008\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.02347\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 2668s 27s/step - loss: 0.0246 - accuracy: 0.9908 - precision: 0.9912 - recall: 0.9904 - val_loss: 0.3040 - val_accuracy: 0.9278 - val_precision: 0.9588 - val_recall: 0.8940\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.02347\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 2688s 27s/step - loss: 0.0262 - accuracy: 0.9911 - precision: 0.9930 - recall: 0.9891 - val_loss: 0.3197 - val_accuracy: 0.9212 - val_precision: 0.9558 - val_recall: 0.8832\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.02347\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 4319s 43s/step - loss: 0.0290 - accuracy: 0.9893 - precision: 0.9907 - recall: 0.9879 - val_loss: 0.3226 - val_accuracy: 0.9194 - val_precision: 0.9448 - val_recall: 0.8908\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.02347\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 4347s 43s/step - loss: 0.0240 - accuracy: 0.9906 - precision: 0.9928 - recall: 0.9883 - val_loss: 0.4358 - val_accuracy: 0.8884 - val_precision: 0.8903 - val_recall: 0.8860\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.02347\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 4317s 43s/step - loss: 0.0301 - accuracy: 0.9887 - precision: 0.9901 - recall: 0.9874 - val_loss: 0.3191 - val_accuracy: 0.9276 - val_precision: 0.9557 - val_recall: 0.8968\n",
      "\n",
      "Epoch 00072: loss did not improve from 0.02347\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 4346s 43s/step - loss: 0.0335 - accuracy: 0.9890 - precision: 0.9915 - recall: 0.9864 - val_loss: 0.2897 - val_accuracy: 0.9262 - val_precision: 0.9397 - val_recall: 0.9108\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.02347\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 4335s 43s/step - loss: 0.0257 - accuracy: 0.9908 - precision: 0.9933 - recall: 0.9882 - val_loss: 0.3268 - val_accuracy: 0.9150 - val_precision: 0.9350 - val_recall: 0.8920\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.02347\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 4357s 44s/step - loss: 0.0267 - accuracy: 0.9906 - precision: 0.9920 - recall: 0.9892 - val_loss: 0.2796 - val_accuracy: 0.9304 - val_precision: 0.9654 - val_recall: 0.8928\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.02347\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 4346s 43s/step - loss: 0.0286 - accuracy: 0.9895 - precision: 0.9891 - recall: 0.9900 - val_loss: 0.2625 - val_accuracy: 0.9322 - val_precision: 0.9331 - val_recall: 0.9312\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.02347\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 4342s 43s/step - loss: 0.0260 - accuracy: 0.9883 - precision: 0.9898 - recall: 0.9869 - val_loss: 0.4267 - val_accuracy: 0.9048 - val_precision: 0.9617 - val_recall: 0.8432\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.02347\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 4342s 43s/step - loss: 0.0283 - accuracy: 0.9897 - precision: 0.9903 - recall: 0.9891 - val_loss: 0.2447 - val_accuracy: 0.9368 - val_precision: 0.9483 - val_recall: 0.9240\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.02347\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 4335s 43s/step - loss: 0.0244 - accuracy: 0.9905 - precision: 0.9923 - recall: 0.9887 - val_loss: 0.3179 - val_accuracy: 0.9216 - val_precision: 0.9474 - val_recall: 0.8928\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.02347\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 4357s 44s/step - loss: 0.0252 - accuracy: 0.9911 - precision: 0.9936 - recall: 0.9885 - val_loss: 0.3880 - val_accuracy: 0.9138 - val_precision: 0.9500 - val_recall: 0.8736\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.02347\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 4374s 44s/step - loss: 0.0251 - accuracy: 0.9912 - precision: 0.9927 - recall: 0.9896 - val_loss: 0.3552 - val_accuracy: 0.9176 - val_precision: 0.9591 - val_recall: 0.8724\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.02347\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 3600s 36s/step - loss: 0.0203 - accuracy: 0.9932 - precision: 0.9945 - recall: 0.9918 - val_loss: 0.3833 - val_accuracy: 0.9126 - val_precision: 0.9264 - val_recall: 0.8964\n",
      "\n",
      "Epoch 00082: loss improved from 0.02347 to 0.02119, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 2676s 27s/step - loss: 0.0258 - accuracy: 0.9905 - precision: 0.9915 - recall: 0.9895 - val_loss: 0.4049 - val_accuracy: 0.9164 - val_precision: 0.9323 - val_recall: 0.8980\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.02119\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 2669s 27s/step - loss: 0.0215 - accuracy: 0.9924 - precision: 0.9913 - recall: 0.9936 - val_loss: 0.3199 - val_accuracy: 0.9306 - val_precision: 0.9476 - val_recall: 0.9116\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.02119\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 2640s 26s/step - loss: 0.0185 - accuracy: 0.9939 - precision: 0.9950 - recall: 0.9927 - val_loss: 0.3597 - val_accuracy: 0.9228 - val_precision: 0.9572 - val_recall: 0.8852\n",
      "\n",
      "Epoch 00085: loss improved from 0.02119 to 0.02018, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 2670s 27s/step - loss: 0.0189 - accuracy: 0.9923 - precision: 0.9936 - recall: 0.9911 - val_loss: 0.3088 - val_accuracy: 0.9340 - val_precision: 0.9536 - val_recall: 0.9124\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.02018\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 2661s 27s/step - loss: 0.0277 - accuracy: 0.9905 - precision: 0.9919 - recall: 0.9890 - val_loss: 0.3238 - val_accuracy: 0.9242 - val_precision: 0.9662 - val_recall: 0.8792\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.02018\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 2670s 27s/step - loss: 0.0271 - accuracy: 0.9895 - precision: 0.9908 - recall: 0.9883 - val_loss: 0.2348 - val_accuracy: 0.9412 - val_precision: 0.9554 - val_recall: 0.9256\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.02018\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 2668s 27s/step - loss: 0.0221 - accuracy: 0.9931 - precision: 0.9949 - recall: 0.9912 - val_loss: 0.4255 - val_accuracy: 0.9142 - val_precision: 0.9508 - val_recall: 0.8736\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.02018\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 2874s 29s/step - loss: 0.0211 - accuracy: 0.9914 - precision: 0.9933 - recall: 0.9895 - val_loss: 0.3274 - val_accuracy: 0.9286 - val_precision: 0.9629 - val_recall: 0.8916\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.02018\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 2677s 27s/step - loss: 0.0216 - accuracy: 0.9912 - precision: 0.9918 - recall: 0.9905 - val_loss: 0.3736 - val_accuracy: 0.9150 - val_precision: 0.9300 - val_recall: 0.8976\n",
      "\n",
      "Epoch 00091: loss improved from 0.02018 to 0.01912, saving model to /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt\n",
      "INFO:tensorflow:Assets written to: /home/sarah/ForthBrainCancer-Dataset/inception_logs/model_inception_v3_20210213-180332.ckpt/assets\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 2694s 27s/step - loss: 0.0220 - accuracy: 0.9913 - precision: 0.9927 - recall: 0.9899 - val_loss: 0.4020 - val_accuracy: 0.9066 - val_precision: 0.9399 - val_recall: 0.8688\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.01912\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 2712s 27s/step - loss: 0.0248 - accuracy: 0.9909 - precision: 0.9913 - recall: 0.9904 - val_loss: 0.3622 - val_accuracy: 0.9164 - val_precision: 0.9668 - val_recall: 0.8624\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.01912\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 2707s 27s/step - loss: 0.0215 - accuracy: 0.9912 - precision: 0.9920 - recall: 0.9904 - val_loss: 0.3012 - val_accuracy: 0.9306 - val_precision: 0.9567 - val_recall: 0.9020\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.01912\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 2702s 27s/step - loss: 0.0241 - accuracy: 0.9925 - precision: 0.9946 - recall: 0.9904 - val_loss: 0.3735 - val_accuracy: 0.9240 - val_precision: 0.9653 - val_recall: 0.8796\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.01912\n",
      "Epoch 96/100\n",
      " 75/100 [=====================>........] - ETA: 10:42 - loss: 0.0235 - accuracy: 0.9917 - precision: 0.9921 - recall: 0.9912"
     ]
    }
   ],
   "source": [
    "now1 = datetime.now()\n",
    "model_type = 'inception_v3'\n",
    "model_hdf5 = '/home/sarah/ForthBrainCancer-Dataset/inception_logs/model_{}_'.format(model_type) + now1.strftime(\"%Y%m%d-%H%M%S\") + '.ckpt'\n",
    "filepath = '/home/sarah/ForthBrainCancer-Dataset/inception_logs/{}.pkl'.format(model_type)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_hdf5, monitor='loss',verbose=1, save_best_only=True)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, \n",
    "                           verbose=1, mode='max', min_lr=1e-5)\n",
    "\n",
    "#all callbacks\n",
    "callbacks = [tensorboard_callback, model_checkpoint, reduce_lr]        \n",
    "history = model.fit(x=train_data.generator(batch_size_neg, batch_size_pos, True, green_layer_only=True, color_normalization_file=color_normalization_file),\n",
    "                    validation_data=val_data.generator(batch_size_neg, batch_size_pos, False, green_layer_only=True, color_normalization_file=color_normalization_file),\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    steps_per_epoch = batches_per_train_epoch,\n",
    "                    validation_steps = batches_per_val_epoch,\n",
    "                    callbacks=callbacks\n",
    "                   )\n",
    "dtme = now1.strftime('%m%d%Y_%H%M%S.pkl')\n",
    "now2 = datetime.now()\n",
    "print('duration: {} - path: {}'.format(now2 - now1, filepath))\n",
    "history_to_pickle(history, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/home/sarah/ForthBrainCancer-Dataset/inception_logs/{}.ckpt'.format(model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDF5_FOLDER = '/home/sarah/ForthBrainCancer-Dataset/testing_CAMELYON16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TissueDataset(HDF5_FOLDER, percentage=0.1, first_part=False, crop_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tf.keras.models import load_model\n",
    "#checkpoint_path = '/home/sarah/ForthBrainCancer-Dataset/inception_logs/inception_v3.hdf5'\n",
    "#model = tf.keras.models.load_model(checkpoint_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_norm = test_data.generator(num_neg=2000, num_pos=2000, data_augm=False, color_normalization_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "#Y, Y_pred = list(), list()\n",
    "for x, y in iter_norm:\n",
    "    y_pred = model.predict(x)\n",
    "    i +=1\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.around(y_pred.flatten())\n",
    "#y_pred = np.around(y_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print('Classification Report')\n",
    "#target_names = ['Cats', 'Dogs', 'Horse']\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score:')\n",
    "print(accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = train_data.generator(num_neg=2000, num_pos=2000, data_augm=False, color_normalization_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "#Y, Y_pred = list(), list()\n",
    "for x, y in iter_train:\n",
    "    y_pred = model.predict(x)\n",
    "    i +=1\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.around(y_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print('Classification Report')\n",
    "#target_names = ['Cats', 'Dogs', 'Horse']\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy score:')\n",
    "print(accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_normalization_file=\"CAMELYON16_color_normalization.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = '/home/sarah/ForthBrainCancer-Dataset/inception_logs/model.hdf5'\n",
    "#model = tf.keras.models.load_model(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iter_norm = test_data.generator(num_neg=1, num_pos=1, data_augm=False, color_normalization_file=color_normalization_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(itera, num_samples=2):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    for x, y in itera:\n",
    "        print(x.shape)\n",
    "        for i in range(num_samples):\n",
    "            ax = plt.subplot(1, num_samples, i + 1)\n",
    "            plt.tight_layout()\n",
    "            ax.set_title('Sample #{} - class {}'.format(i, y[i]))\n",
    "            ax.imshow(x[i])\n",
    "            ax.axis('off') \n",
    "        break # generate yields infinite random samples, so we stop after first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(iter_norm, num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x, y in iter_norm:\n",
    "    pred = model.predict(x)\n",
    "    y0 = 0 if pred[0]< 0.5 else 1\n",
    "    y1 = 0 if pred[1]< 0.5 else 1\n",
    "    print(y, np.array([y0, y1]))\n",
    "    i +=1\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "def np_to_pil(np_img):\n",
    "    \"\"\"\n",
    "    Convert a NumPy array to a PIL Image.\n",
    "    Args:\n",
    "    np_img: The image represented as a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "     The NumPy array converted to a PIL Image.\n",
    "    \"\"\"\n",
    "    if np_img.dtype == \"bool\":\n",
    "        np_img = np_img.astype(\"uint8\") * 255\n",
    "    elif np_img.dtype == \"float64\":\n",
    "        np_img = (np_img * 255).astype(\"uint8\")\n",
    "    return Image.fromarray(np_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[1])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features_figure(feat_model, path_image):\n",
    "    # Example of features on the last conv2D layer - 83 according to model.summary\n",
    "    model = Model(inputs=feat_model.inputs, outputs=feat_model.layers[83].output)\n",
    "    # load the image with the required shape\n",
    "    img = image.load_img(path_image, target_size=(224, 224))\n",
    "    # convert the image to an array\n",
    "    img = image.img_to_array(img)\n",
    "    # expand dimensions so that it represents a single 'sample'\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    # prepare the image (e.g. scale pixel values for the vgg)\n",
    "    img = preprocess_input(img)\n",
    "    # get feature map for first hidden layer\n",
    "    feature_maps = model.predict(img)\n",
    "    # plot all 64 maps in an 8x8 squares\n",
    "    square = 8\n",
    "    ix = 1\n",
    "    f, axs = plt.subplots(2,2,figsize=(15,15))\n",
    "    for _ in range(square):\n",
    "        for _ in range(square):\n",
    "            # specify subplot and turn of axis\n",
    "            ax = plt.subplot(square, square, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # plot filter channel in grayscale\n",
    "            plt.imshow(feature_maps[0, :, :, ix-1]) #, cmap='gray')\n",
    "            ix += 1\n",
    "    # show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_features_figure(model, '/home/sarah/ForthBrainCancer-Dataset/inception_logs/normal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_features_figure(model, '/home/sarah/ForthBrainCancer-Dataset/inception_logs/tumor.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
